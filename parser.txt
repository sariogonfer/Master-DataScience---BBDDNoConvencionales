Para leer el XML abrimos una shell de pyspark con el paquete de java spark-xml:

pyspark --packages com.databricks:spark-xml_2.11:0.4.1

Desde pyspark ejutamos para cada tipo de publicación el siguiente mandato:

df = spark.read.format('com.databricks.spark.xml').option("rowTag", "incollection").load('./dblp.xml')

Al no definiri el esquema, intenta "adivinarlo" por si mimso. Esto causa problemas al intentar
parsear algunos campos. Por ello lo mejor es definir nuestro propio esquema definiendo todos los campos
como strings. Ya que solo utilizamos este dataframe para transformar el xml a JSON, con el tipo string
nos es suficiente. Para hacerlo más automático, creamos el esquema apartir del df ya leido.

from pyspark.sql.types import StructField, StringType, StructType

custom_types = []
for c in df.columns[1:]:
    custom_types.append(StructField(str(c), StringType(), nullable=True))

custom_schema = StructType(custom_types)

Por último para escribir el json:

df.write.json('./incollection')



Para sacar los CSVs necesarios para Neo4J, desde pyspark ejecutaremos lo siguiente (ejemplo incollections):

Para extraer los CSVs, el código anda en xml_to_neo4j.py

Para importar estos csv, se utiliza neo4j-admin import.

neo4j-admin import -nodes:Publication "publications.header.csv,publications/part.*" -nodes:Author "authors.header.csv,authors/part.*" -relationships:Writed "rels.header.csv,rels/part.*" --ignore-duplicate-nodes=true

Las cabeceras son:

authors.csv: name:ID
publications.csv: key:ID,title,year:int,:LABEL
rels.csv: :START_ID,:END_ID

