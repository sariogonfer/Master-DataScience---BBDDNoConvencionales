Para leer el XML abrimos una shell de pyspark con el paquete de java spark-xml:

pyspark --packages com.databricks:spark-xml_2.11:0.4.1

Desde pyspark ejutamos para cada tipo de publicación el siguiente mandato:

df = spark.read.format('com.databricks.spark.xml').option("rowTag", "incollection").load('./dblp.xml')

Al no definiri el esquema, intenta "adivinarlo" por si mimso. Esto causa problemas al intentar
parsear algunos campos. Por ello lo mejor es definir nuestro propio esquema definiendo todos los campos
como strings. Ya que solo utilizamos este dataframe para transformar el xml a JSON, con el tipo string
nos es suficiente. Para hacerlo más automático, creamos el esquema apartir del df ya leido.

from pyspark.sql.types import StructField, StringType, StructType

custom_types = []
for c in df.columns[1:]:
    custom_types.append(StructField(str(c), StringType(), nullable=True))

custom_schema = StructType(custom_types)

Por último para escribir el json:

df.write.json('./incollection')



Para sacar los CSVs necesarios para Neo4J, desde pyspark ejecutaremos lo siguiente (ejemplo incollections):

from pyspark.sql.functions import explode, lit, translate
df = spark.read.format('com.databricks.spark.xml').option("rowTag", "incollection").load('./dblp.xml')

df.withColumn('LABEL', lit('publication;incollections')).withColumn('id', translate('_key', '/', '_')).select('_key', 'year', 'title', 'LABEL').write.csv('./csv/incollections')

df.withColumn('a', explode('author')).withColumn('id', translate('_key', '/', '_')).withColumn('TYPE', lit('WRITES')).select('id', 'a._VALUE', 'TYPE').write.csv('./csv/incollection_author_rel')

df.withColumn('a', explode('author')).select('a._VALUE').distinct().withColumn('LABEL', lit('author')).select('_VALUE', 'LABEL').write.csv('./csv/authors')


Luego hay que meterse y juntar los csv:

cat part* > lo_que_sea.csv

Y escribir las cabeceras:
Para autores: name:ID,:LABEL
Para publicaciones: key:ID,year:int,title,:LABEL
Para relaciones: :START_ID,:END_ID,:TYPE

