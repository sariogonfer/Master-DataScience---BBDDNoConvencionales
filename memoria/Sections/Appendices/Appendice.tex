\section{Creación de vistas}\label{sec:parser}

De forma similar que en bases de datos relaciones, es posible crear vistas. Las vistas son consultas sobre diferentes tablas a través de los campos que designemos. Una vez creada una vista, las consultas se realizan de la misma forma que si fuese una colección, pudiendo filtrar por alguno de sus campos.

En nuestro caso, hemos creado la vista \textit{publications\_extended}. Esta vista, a partir de la colección \textit{authors} que contiene un documento por cada autor y algunos campos básicos de cada tipo de documento, incluyendo el campo \_id, cruzando con el resto de colecciones con diferentes publicaciones, conseguimos unificar toda la información extendida sobre una vista. Al incluir nuevos documentos en las colecciones de origen, la vista automáticamente devuelve dichos registros. La definición es la siguiente:

\begin{minted}[
frame=single]{js}
db.createView (
  "publications_extended",
  "authors",
  [
    { $lookup: { 
      from: "articles", localField: "articles._id",
    foreignField: "_id", as: "articles_extended" } },
    { $lookup: { 
      from: "incollections", localField: "incollections._id", 
    foreignField: "_id", as: "incollections_extended" } },
    { $lookup: { 
      from: "inproceedings", localField: "inproceedings._id",
    foreignField: "_id", as: "inproceedings_extended" } },
    { $project: { 
      articles_extended: 1, incollections_extended: 1,
    inproceedings_extended: 1}}
  ]
)
\end{minted}

Una vez más, comprobamos que se ha ejecutado el comando correctamente:

\begin{minted}[
frame=single]{js}
/* 1 */
{
    "ok" : 1.0
}
\end{minted}

A partir de este momento, ya se puede ejecutar cualquier consulta a la vista como si de una tabla se tratase. Por ejemplo, podemos ejecutar la consulta del ejercicio 1, \textit{Listado de todas las publicaciones de un autor determinado}:


\begin{minted}[
frame=single]{js}
db.publications_extended.aggregate([
    { $match : { _id : "Chin-Wang Tao" } },
    { $project: {publication: {$concatArrays:
        ["$articles_extended.title", "$incollections_extended.title", "$inproceedings_extended.title"]}}
    }
])
\end{minted}

Como resultado, obtenemos el mismo resultado (aunque el orden puede variar):

\begin{minted}[
frame=single]{js}
/* 1 */
{
    "_id" : "Chin-Wang Tao",
    "publication" : [ 
        "On the robustness of stability of fuzzy control systems.", 
        "Adaptive fuzzy PIMD controller for systems with uncertain deadzones.", 
        "Design and analysis of region-wise linear fuzzy controllers.", 
        "Adaptive Fuzzy Switched Swing-Up and Sliding Control for the Double-Pendulum-and-Cart System.", 
        "An Approximation of Interval Type-2 Fuzzy Controllers Using Fuzzy Ratio Switching Type-1 Fuzzy Controllers.", 
        "Adaptive fuzzy terminal sliding mode controller for linear systems with mismatched time-varying uncertainties.", 
        "A reduction approach for fuzzy rule bases of fuzzy controllers.", 
        "Fuzzy Sliding-Mode Formation Control for Multirobot Systems: Design and Implementation.", 
        "Segmentation of Psoriasis Vulgaris Images Using Multiresolution-Based Orthogonal Subspace Techniques.", 
        "Fuzzy control for linear plants with uncertain output backlashes.", 
        "Iris Recognition Using Possibilistic Fuzzy Matching on Local Features.", 
        "Radial Basis Function Networks With Linear Interval Regression Weights for Symbolic Interval Data.", 
        "Adaptive fuzzy sliding mode controller for linear systems with mismatched time-varying uncertainties.", 
        "Design of fuzzy controllers with adaptive rule insertion.", 
        "Flexible complexity reduced PID-like fuzzy controllers.", 
        "Interval competitive agglomeration clustering algorithm.", 
        "Fuzzy sliding-mode control for ball and beam system with fuzzy ant colony optimization.", 
        "Hybrid robust approach for TSK fuzzy modeling with outliers.", 
        "Design of a Fuzzy Controller With Fuzzy Swing-Up and Parallel Distributed Pole Assignment Schemes for an Inverted Pendulum and Cart System.", 
        "Nested design of fuzzy controllers with partial fuzzy rule base.", 
        "Fuzzy hierarchical swing-up and sliding position controller for the inverted pendulum-cart system.", 
        "Design of a parallel distributed fuzzy LQR controller for the twin rotor multi-input multi-output system.", 
        "Fuzzy adaptive approach to fuzzy controllers with spacial model.", 
        "Simplified type-2 fuzzy sliding controller for wing rock system.", 
        "Unsupervised fuzzy clustering with multi-center clusters.", 
        "An advanced fuzzy controller.", 
        "A Novel Approach to Implement Takagi-Sugeno Fuzzy Models.", 
        "Fuzzy Swing-Up and Fuzzy Sliding-Mode Balance Control for a Planetary-Gear-Type Inverted Pendulum.", 
        "A Design of a DC-AC Inverter Using a Modified ZVS-PWM Auxiliary Commutation Pole and a DSP-Based PID-Like Fuzzy Control.", 
        "Robust fuzzy control for a plant with fuzzy linear model.", 
        "Comments on \"Reduction of fuzzy rule base via singular value decomposition\".", 
        "A Novel Fuzzy-Sliding and Fuzzy-Integral-Sliding Controller for the Twin-Rotor Multi-Input-Multi-Output System.", 
        "Robust L", 
        "A Fuzzy Logic Approach to Target Tracking", 
        "Index compression for vector quantisation using modified coding tree assignment scheme.", 
        "Hybrid SVMR-GPR for modeling of chaotic time series systems with noise and outliers.", 
        "Parallel Distributed Fuzzy Sliding Mode Control for Nonlinear Mismatched Uncertain Systems.", 
        "An approach for the robustness comparison between piecewise linear ", 
        "Interval fuzzy sliding-mode formation controller design.", 
        "Simplification of a fuzzy mechanism with rule combination.", 
        "Robust control of systems with fuzzy representation of uncertainties.", 
        "Checking identities is computationally intractable NP-hard and therefore human provers will always be needed.", 
        "A kernel-based core growing clustering method.", 
        "An Alternative Type Reduction Approach Based on Information Combination with Interval Operations for Interval-Valued Fuzzy Sliding Controllers.", 
        "Designs and analyses of various fuzzy controllers with region-wise linear PID subcontrollers.", 
        "Adaptive Bound Reduced-Form Genetic Algorithms for B-Spline Neural Network Training.", 
        "A New Neuro-Fuzzy Classifier with Application to On-Line Face Detection and Recognition.", 
        "Statistical and Dempster-Shafer Techniques in Testing Structural Integrity of Aerospace Structures.", 
        "Embedded support vector regression on Cerebellar Model Articulation Controller with Gaussian noise.", 
        "Iris Recognition Using Gabor Filters and the Fractal Dimension.", 
        "A two-stage design of adaptive fuzzy controllers for time-delay systems with unknown models.", 
        "Errata: Iris recognition using Gabor filters optimized by the particle swarm algorithm.", 
        "Texture classification using a fuzzy texture spectrum and neural networks.", 
        "Iris recognition using Gabor filters optimized by the particle swarm algorithm.", 
        "iPhone as Multi-CAM and Multi-viewer.", 
        "Sliding control for linear uncertain systems.", 
        "FPGA implementation of improved ant colony optimization algorithm for path planning.", 
        "Medical image compression using principal component analysis.", 
        "Design of a Kinect Sensor Based Posture Recognition System.", 
        "A simplified interval type-2 fuzzy CMAC.", 
        "iOS based Multi-Cloud Manage System.", 
        "A Novel Approach to Implement Takagi-Sugeno Fuzzy Models.", 
        "Path-planning using the behavior cost and the path length with a multi-resolution scheme.", 
        "Iris recognition using Gabor filters optimized by the particle swarm technique.", 
        "Hybrid robust LS-SVMR with outliers for MIMO system.", 
        "Design of A Two-Stage Adaptive Fuzzy Controller.", 
        "Face Detection Using Eigenface and Neural Network.", 
        "Robust control of the mismatched systems with the fuzzy integral sliding controller.", 
        "Support Vector Regression for Controller Approximation.", 
        "Robust clustering algorithm for the symbolic interval-values data with outliers."
    ]
}
\end{minted}

\section{Parseador de XML a JSON y CSV}\label{sec:parser}

Para realizar el parseo de los datos hemos decidido hacer uso de la biblioteca pyspark sobre Python 3 para aprovechar la gestión de la memoria que hace. ¿Por qué de esta decisión?. Intentar procesar el fichero \gls{XML} directamente con un script de Python sin uso de la gestión de memoria de las bibliotecas nativas de Spark provocaba que las máquinas donde se ejecuba acabaran bloqueandose debido a que se llegaba al límite de la capacidad de la memoria, y por lo que se ha podido comprobar, los paquetes encargados de este parseo no gestionan correctamente estos escenarios.

Otra alternativa era dividir el fichero XML en bloques y procesarlos de manera individual. Así se reduce la carga de datos en memoria y se consigue no llegar al límite de la máquina. Esta técnica puede se más compleja ya que requiere leer el fichero línea a línea y procesarla para saber si podemos ``cortar'' sobre ella no separar un elemento valido (un \textit{articles} en dos bloques.

Debido a la facilidad de uso que nos ofrece PySpark, y aprovechando que es contenido de otra materia de este master, lo hemos visto como la opción más recomendable.

\subsection{Pasos}

Para leer el XML simplemente debemos ejecutar el script adjunto parser.py. En primer lugar, hemos definido las bibliotecas necesarias a utilizar:

\begin{minted}[
frame=single]{python}
import os
import pyspark
from tempfile import NamedTemporaryFile
import html

from pyspark.sql import SparkSession, functions
from pyspark.sql.types import StringType, StructType, StructField, ArrayType
from pyspark.sql.functions import explode, concat_ws, lit, translate
\end{minted}
 

En segundo lugar, hemos definido el uso de la biblioteca externa de parseo de xml. Para ello, debemos modificar la variable de entorno PYSPARK_SUBMIT_ARGS:

\begin{minted}[
frame=single]{python}
packages = "com.databricks:spark-xml_2.11:0.4.1"

os.environ["PYSPARK_SUBMIT_ARGS"] = (
    "--packages {0} pyspark-shell".format(packages)
)
\end{minted}

Una vez definida la variable de entorno, inicializamos la variable sc, que contendrá de aquí en adelante el SparkContext, objeto necesario para gestionar las llamadas en Spark. Además, definimos el esquema que nos vamos a encontrar mediante la tipología de datos de cada campo:

\begin{minted}[
frame=single]{python}
spark = (SparkSession.builder
    .master("local[4]")
    .config("spark.driver.cores", 1)
    .appName("Getting graph information")
    .getOrCreate() )
    
sc = spark.sparkContext
\end{minted}

Al no tener un esquema definido, es el propio paquete de \textbf{Spark} el que intenta inferirlo por si mismo. Esto causa problemas al intentar parsear algunos campos. Por ello lo mejor es definir nuestro propio esquema definiendo todos los campos como si se tratasen de ``Strings''. Ya que solo utilizamos este dataframe para transformar el \textit{XML} a \textit{JSON} y \textit{CSV}, y no vamos a realizar ningún tipo de validación u operación con el \textbf{Datafame} que se crea, tratar todos los campos como ``Strings'' es más que suficiente.

\begin{minted}[
frame=single]{python}
schema = StructType([
  StructField("_key", StringType()),
  StructField("title", StringType()),
  StructField("year", StringType()),
  StructField("author", ArrayType(
      StructType([
          StructField("_VALUE", StringType())
      ])
   ))
])
\end{minted}

Una vez definido, procedemos a abrir el fichero dblp.xml y almacenamos eliminando caracteres de escape en html cada línea en un fichero temporal.

\begin{minted}[
frame=single]{python}
with open('./dblp.xml') as source, NamedTemporaryFile('w') as unescaped_src:
    for line in source:
        unescaped_src.write(html.unescape(line))
\end{minted}
        
        
Posteriormente, recuperamos cada tipo de ejecución mediante el siguiente mandato:

\begin{minted}[
frame=single]{python}
incollections_df = spark.read.format('com.databricks.spark.xml').option(
        "rowTag", "incollection").option('charset', "UTF-8").schema(
        schema).load(unescaped_src.name)
inproceedings_df = spark.read.format('com.databricks.spark.xml').option(
        "rowTag", "inproceedings").option('charset',
        "UTF-8").schema(schema).load(unescaped_src.name)
articles_df = spark.read.format('com.databricks.spark.xml').option("rowTag",
        "article").option('charset', "UTF-8").schema(schema).load(
        unescaped_src.name)
\end{minted}
        
Una vez cargados los DataFrames en función de la tipología de publicación, volcamos cada tipo en 3 ficheros json para la carga desde mongoimport:


\begin{minted}[
frame=single]{python}
incollections_df.write.option("charset", "UTF-8").json('./json/incollections')
inproceedings_df.write.option("charset", "UTF-8").json('./json/inproceedings')
articles_df.write.option("charset", "UTF-8").json('./json/articles')
\end{minted}

Por último, para la carga de datos en Neo4j vamos a generar 3 ficheros CSV, el primero de ellos con todas las publicaciones, el segundo con todos los autores y el tercero con todas las relaciones entre ellos:

\begin{minted}[
frame=single]{python}
publications_df =   incollections_df.withColumn('LABEL',lit('Incollection')).union(
                    inproceedings_df.withColumn('LABEL',lit('Inproceeding'))).union(
                    articles_df.withColumn('LABEL', lit('Article')))

publications_df = publications_df.filter(publications_df._key.isNotNull())

publications_df.withColumn('id', translate('_key', '/', '_')).select('id',
    'title', 'year', 'LABEL').write.option('escape', '"').csv(
    './csv/publications')

publications_df.withColumn('_author', explode('author._VALUE')).select(
    '_author').write.option('escape', '"').csv('./csv/authors')

publications_df.withColumn('start', explode('author._VALUE')).withColumn(
    'end', translate('_key', '/', '_')).select('start', 'end').write.option(
        'escape', '"').csv('./csv/rels')
\end{minted}


Hemos encapsulado la ejecución de este script mediante simplemente la ejecución del script adjunto "parse.sh" mediante su ejecución:

\begin{minted}[
frame=single]{js}
./parse.sh
\end{minted}

Ese fichero eliminará los directorios donde se almacenarán los ficheros de salida, ejecutará el script y agrupará los ficheros temporales \textit{json} en un fichero completo por cada tipología. Para el caso de \textit{CSV}, no ha sido necesario agrupar puesto que en la importación a neo4j indicamos que se recuperen todos los ficheros part-* (ficheros generados por spark).
