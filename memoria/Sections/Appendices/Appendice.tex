\section{Creación de vistas}\label{sec:parser}

De forma similar que en bases de datos relaciones, es posible crear vistas. Las vistas son consultas sobre diferentes tablas a través de los campos que designemos. Una vez creada una vista, las consultas se realizan de la misma forma que si fuese una colección, pudiendo filtrar por alguno de sus campos.

En nuestro caso, hemos creado la vista \textit{publications\_extended}. Esta vista, a partir de la colección \textit{authors} que contiene un documento por cada autor y algunos campos básicos de cada tipo de documento, incluyendo el campo \_id, cruzando con el resto de colecciones con diferentes publicaciones, conseguimos unificar toda la información extendida sobre una vista. Al incluir nuevos documentos en las colecciones de origen, la vista automáticamente devuelve dichos registros. La definición es la siguiente:

\begin{minted}[
frame=single]{js}
db.createView (
  "publications_extended",
  "authors",
  [
    { $lookup: { 
      from: "articles", localField: "articles._id",
    foreignField: "_id", as: "articles_extended" } },
    { $lookup: { 
      from: "incollections", localField: "incollections._id", 
    foreignField: "_id", as: "incollections_extended" } },
    { $lookup: { 
      from: "inproceedings", localField: "inproceedings._id",
    foreignField: "_id", as: "inproceedings_extended" } },
    { $project: { 
      articles_extended: 1, incollections_extended: 1,
    inproceedings_extended: 1}}
  ]
)
\end{minted}

Una vez más, comprobamos que se ha ejecutado el comando correctamente:

\begin{minted}[
frame=single]{js}
/* 1 */
{
    "ok" : 1.0
}
\end{minted}

A partir de este momento, ya se puede ejecutar cualquier consulta a la vista como si de una tabla se tratase. Por ejemplo, podemos ejecutar la consulta del ejercicio 1, \textit{Listado de todas las publicaciones de un autor determinado}:


\begin{minted}[
frame=single]{js}
db.publications_extended.aggregate([
    { $match : { _id : "Chin-Wang Tao" } },
    { $project: {publication: {$concatArrays:
        ["$articles_extended.title", "$incollections_extended.title", "$inproceedings_extended.title"]}}
    }
])
\end{minted}

Como resultado, obtenemos el mismo resultado (aunque el orden puede variar):

\begin{minted}[
frame=single]{js}
/* 1 */
{
    "_id" : "Chin-Wang Tao",
    "publication" : [ 
        "On the robustness of stability of fuzzy control systems.", 
        "Adaptive fuzzy PIMD controller for systems with uncertain deadzones.", 
        "Design and analysis of region-wise linear fuzzy controllers.", 
        "Adaptive Fuzzy Switched Swing-Up and Sliding Control for the Double-Pendulum-and-Cart System.", 
        "An Approximation of Interval Type-2 Fuzzy Controllers Using Fuzzy Ratio Switching Type-1 Fuzzy Controllers.", 
        "Adaptive fuzzy terminal sliding mode controller for linear systems with mismatched time-varying uncertainties.", 
        "A reduction approach for fuzzy rule bases of fuzzy controllers.", 
        "Fuzzy Sliding-Mode Formation Control for Multirobot Systems: Design and Implementation.", 
        "Segmentation of Psoriasis Vulgaris Images Using Multiresolution-Based Orthogonal Subspace Techniques.", 
        "Fuzzy control for linear plants with uncertain output backlashes.", 
        "Iris Recognition Using Possibilistic Fuzzy Matching on Local Features.", 
        "Radial Basis Function Networks With Linear Interval Regression Weights for Symbolic Interval Data.", 
        "Adaptive fuzzy sliding mode controller for linear systems with mismatched time-varying uncertainties.", 
        "Design of fuzzy controllers with adaptive rule insertion.", 
        "Flexible complexity reduced PID-like fuzzy controllers.", 
        "Interval competitive agglomeration clustering algorithm.", 
        "Fuzzy sliding-mode control for ball and beam system with fuzzy ant colony optimization.", 
        "Hybrid robust approach for TSK fuzzy modeling with outliers.", 
        "Design of a Fuzzy Controller With Fuzzy Swing-Up and Parallel Distributed Pole Assignment Schemes for an Inverted Pendulum and Cart System.", 
        "Nested design of fuzzy controllers with partial fuzzy rule base.", 
        "Fuzzy hierarchical swing-up and sliding position controller for the inverted pendulum-cart system.", 
        "Design of a parallel distributed fuzzy LQR controller for the twin rotor multi-input multi-output system.", 
        "Fuzzy adaptive approach to fuzzy controllers with spacial model.", 
        "Simplified type-2 fuzzy sliding controller for wing rock system.", 
        "Unsupervised fuzzy clustering with multi-center clusters.", 
        "An advanced fuzzy controller.", 
        "A Novel Approach to Implement Takagi-Sugeno Fuzzy Models.", 
        "Fuzzy Swing-Up and Fuzzy Sliding-Mode Balance Control for a Planetary-Gear-Type Inverted Pendulum.", 
        "A Design of a DC-AC Inverter Using a Modified ZVS-PWM Auxiliary Commutation Pole and a DSP-Based PID-Like Fuzzy Control.", 
        "Robust fuzzy control for a plant with fuzzy linear model.", 
        "Comments on \"Reduction of fuzzy rule base via singular value decomposition\".", 
        "A Novel Fuzzy-Sliding and Fuzzy-Integral-Sliding Controller for the Twin-Rotor Multi-Input-Multi-Output System.", 
        "Robust L", 
        "A Fuzzy Logic Approach to Target Tracking", 
        "Index compression for vector quantisation using modified coding tree assignment scheme.", 
        "Hybrid SVMR-GPR for modeling of chaotic time series systems with noise and outliers.", 
        "Parallel Distributed Fuzzy Sliding Mode Control for Nonlinear Mismatched Uncertain Systems.", 
        "An approach for the robustness comparison between piecewise linear ", 
        "Interval fuzzy sliding-mode formation controller design.", 
        "Simplification of a fuzzy mechanism with rule combination.", 
        "Robust control of systems with fuzzy representation of uncertainties.", 
        "Checking identities is computationally intractable NP-hard and therefore human provers will always be needed.", 
        "A kernel-based core growing clustering method.", 
        "An Alternative Type Reduction Approach Based on Information Combination with Interval Operations for Interval-Valued Fuzzy Sliding Controllers.", 
        "Designs and analyses of various fuzzy controllers with region-wise linear PID subcontrollers.", 
        "Adaptive Bound Reduced-Form Genetic Algorithms for B-Spline Neural Network Training.", 
        "A New Neuro-Fuzzy Classifier with Application to On-Line Face Detection and Recognition.", 
        "Statistical and Dempster-Shafer Techniques in Testing Structural Integrity of Aerospace Structures.", 
        "Embedded support vector regression on Cerebellar Model Articulation Controller with Gaussian noise.", 
        "Iris Recognition Using Gabor Filters and the Fractal Dimension.", 
        "A two-stage design of adaptive fuzzy controllers for time-delay systems with unknown models.", 
        "Errata: Iris recognition using Gabor filters optimized by the particle swarm algorithm.", 
        "Texture classification using a fuzzy texture spectrum and neural networks.", 
        "Iris recognition using Gabor filters optimized by the particle swarm algorithm.", 
        "iPhone as Multi-CAM and Multi-viewer.", 
        "Sliding control for linear uncertain systems.", 
        "FPGA implementation of improved ant colony optimization algorithm for path planning.", 
        "Medical image compression using principal component analysis.", 
        "Design of a Kinect Sensor Based Posture Recognition System.", 
        "A simplified interval type-2 fuzzy CMAC.", 
        "iOS based Multi-Cloud Manage System.", 
        "A Novel Approach to Implement Takagi-Sugeno Fuzzy Models.", 
        "Path-planning using the behavior cost and the path length with a multi-resolution scheme.", 
        "Iris recognition using Gabor filters optimized by the particle swarm technique.", 
        "Hybrid robust LS-SVMR with outliers for MIMO system.", 
        "Design of A Two-Stage Adaptive Fuzzy Controller.", 
        "Face Detection Using Eigenface and Neural Network.", 
        "Robust control of the mismatched systems with the fuzzy integral sliding controller.", 
        "Support Vector Regression for Controller Approximation.", 
        "Robust clustering algorithm for the symbolic interval-values data with outliers."
    ]
}
\end{minted}

\section{Parseador de XML a JSON}\label{sec:parser}

Para realizar el parseo de los datos hemos decidido hacer uso de PySpark para aprovechar la gestión de la memoria que hace. ¿Por qué de esta decisión?. Intentar procesar el fichero \gls{XML} directamente con un script de Python provocaba que las máquinas donde se ejecuba acabaran bloqueandose debido a que se llegaba al límite de la capacidad de la memoria, y por lo que se ha podido comprobar, los paquetes encargados de este parseo no gestionan correctamente estos escenarios.

Otra alternativa era dividir el fichero XML en bloques y procesarlos de manera individual. Así se reduce la carga de datos en memoria y se consigue no llegar al límite de la máquina. Esta técnica puede se más compleja ya que requiere leer el fichero línea a línea y procesarla para saber si podemos ``cortar'' sobre ella no separar un elemento valido (un \textit{articles} en dos bloques.

Debido a la facilidad de uso que nos ofrece PySpark, y aprovechando que es conetenido de otra materia de este master, lo hemos visto como la opción más recomendable.

\subsection{Pasos}

Para leer el XML abrimos una shell de pyspark con el paquete de java spark-xml:

\begin{minted}[
frame=single]{js}
pyspark --packages com.databricks:spark-xml_2.11:0.4.1
\end{minted}

Desde pyspark ejutamos para cada tipo de publicación el siguiente mandato:

\begin{minted}[
frame=single]{python}
df = spark.read.format('com.databricks.spark.xml').option("rowTag",
  "incollection").load('./dblp.xml')
\end{minted}

Al no tener un esquema definido, es el propio paquete de \textbf{Spark} el que intenta inferirlo por si mismo. Esto causa problemas al intentar parsear algunos campos. Por ello lo mejor es definir nuestro propio esquema definiendo todos los campos como si se tratasen de ``Strings''. Ya que solo utilizamos este dataframe para transformar el \textit{XML} a \textit{JSON}, y no vamos a realizar ningún tipo de validación u operación con el \textbf{Datafame} que se crea, tratar todos los campos como ``Strings'' es más que suficiente.

\begin{minted}[
frame=single]{python}
from pyspark.sql.types import StructField, StringType, StructType

custom_types = []
for c in df.columns[1:]:
  custom_types.append(StructField(str(c), StringType(), nullable=True))

custom_schema = StructType(custom_types)
\end{minted}

Una vez que tengamos creado el \textbf{DataFrame}, solo nos quedará volcarlo como fichero, o más bien, como múltiples ficheros, ya que por el mecanismo interno de funcionamiento de Spark, la salida sera escrita en múltiples ficheros. Tendremos por tanto que indicarla a la función \textit{json()} el directorio donde queremos que vuelque estos ficheros.

\begin{minted}[
frame=single]{python}
df.write.json('./incollection')
\end{minted}

Una vez terminado, nos encontrameros que dentro del directorio anterior, se han creado varios ficheros cuyos nombres tiene el formato \textit{PARTXXXXX\_\*.json}. Para unirlo todos en uno solo (en caso de querer hacerlo), podemos usar las \textbf{tools} básicas que nos ofrece el ofrece el sistema operativo, en nuestro caso, una distribución basada en \textbf{Debian}:

\begin{minted}[
frame=single]{bash}
cat PART* > out.json
\end{minted}